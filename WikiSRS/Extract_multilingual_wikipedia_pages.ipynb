{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Wikipages in different languages\n",
    "\n",
    "In this notebook, wikipedia pages in 6 different languages (english, arabic, spanish, french, portuguese and russian) for each wiki title in (wikipediaSimilarity353, WikiSRS_relatedness, andWikiSRS_similarity) datasets, and then save the output pages for each dataset in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtzMVyiM1yH2",
    "outputId": "ee13c8d4-738b-4045-8d08-b0e2b2857e56"
   },
   "outputs": [],
   "source": [
    "!pip3 install wikipedia-api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0r818twrgOJ",
    "outputId": "c5643e5b-26f6-4efa-da53-f622198a4d4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "import numpy\n",
    "import wikipediaapi\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadingWikiPages() : \n",
    "    def __init__(self, dataset_name = None):\n",
    "        self.headers = requests.utils.default_headers()\n",
    "        self.headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "        self.wikipedia = wikipediaapi.Wikipedia(language='en',extract_format=wikipediaapi.ExtractFormat.WIKI, headers=self.headers)\n",
    "        self.dataset = self.read_data_file(dataset_name)\n",
    "        self.wiki_languages = ['en','ar','es','fr','ru','pt']\n",
    "        self.wiki_pages = self.get_wikipedia_pages(dataset_name)\n",
    "\n",
    "    def read_data_file(self, dataset_name):\n",
    "        '''\n",
    "        This method reads the dataset file, and also rename some columns names to be consistent with other datasets\n",
    "        Input : name of the dataset, it could be wikipediaSimilarity353, or WikiSRS_relatedness or WikiSRS_similarity\n",
    "        Output: a dataframe fot the corresponding dataset name\n",
    "        '''\n",
    "        \n",
    "        if dataset_name == \"wikipediaSimilarity353\":\n",
    "            data = pd.read_csv(\"data/wikipediaSimilarity353.csv\")\n",
    "            data['titleA'] = data['titleA'].replace(['Production, costs, and pricing'],'Production')#no wikipedia page for 'Production, costs, and pricing'\n",
    "        elif dataset_name == \"WikiSRS_relatedness\" or dataset_name == \"WikiSRS_similarity\":\n",
    "            data = pd.read_csv(\"data/\"+dataset_name+\".csv\", sep='\\t') \n",
    "            data = data.drop(['RawScores', 'StdDev'], axis = 1)\n",
    "            data.rename(columns = {'Term1':'termA', 'String1':'titleA',\n",
    "                                   'Term2':'termB', 'String2':'titleB',\n",
    "                                   'Mean' :'relatedness'}, inplace = True)\n",
    "        return data\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        '''\n",
    "        This method does a light text cleaning, such as removing punctuation and special characters like \\ and \\n\n",
    "        '''\n",
    "        \n",
    "        text = re.sub(\"\\n\", \"\", text)\n",
    "        text = re.sub(\"\\'\", \"\", text)\n",
    "        new_text = re.sub('[!*(-);\\':\"]', \"\", text)\n",
    "        return new_text\n",
    "\n",
    "    def save_data_in_file(self, dataset, dataset_name):\n",
    "        '''\n",
    "        This method saves the dataframe in a file \n",
    "        '''\n",
    "        \n",
    "        data = pd.DataFrame(dataset)\n",
    "        data.to_csv('data/multilingual_'+dataset_name+'.csv')\n",
    "        return\n",
    "    \n",
    "    def get_wikipedia_pages(self, dataset_name):\n",
    "        '''\n",
    "        This method extract the english wikipedia page for each entity title in the dataset.\n",
    "        And, get the page in other langauges.\n",
    "        Input : dataset name.\n",
    "        output : a list of wikipedia pages in different languages for all entities in the datset. \n",
    "        '''\n",
    "        \n",
    "        pages = []\n",
    "        pages_titles = []\n",
    "        for index, row in self.dataset.iterrows():\n",
    "            if row['titleA'] not in pages_titles :\n",
    "                page1 = self.wikipedia.page(row['titleA'])\n",
    "                try:\n",
    "                    page1_content = page1.text\n",
    "                except:\n",
    "                    time.sleep(15)\n",
    "                    print(\"Try again after sleeping for 15 seconds\")\n",
    "                    page1_content = page1.text\n",
    "                if len(page1_content) > 1000: \n",
    "                    pages.append({\"title\": page1.title, \"lang\": \"en\", \"content\": self.clean_text(page1_content)})\n",
    "                    pages_titles.append(row['titleA'])\n",
    "                    soup = BeautifulSoup(urlopen(page1.fullurl))\n",
    "                    links = [(el.get('lang'), el.get('title')) for el in soup.select('li.interlanguage-link > a')]\n",
    "                    for lang, title in links:\n",
    "                        if lang in self.wiki_languages:\n",
    "                            page_title = title.split(u' – ')[0]\n",
    "                            wiki1 = wikipediaapi.Wikipedia(language=lang,extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "                            page = wiki1.page(page_title)\n",
    "                            try:\n",
    "                                p_content = page.text\n",
    "                            except:\n",
    "                                time.sleep(15)\n",
    "                                print(\"Try again after sleeping for 15 seconds\")\n",
    "                                p_content = page.text\n",
    "                            if len(p_content)>1000:\n",
    "                                pages.append({\"title\": page1.title,\"lang\": lang, \"content\": p_content})\n",
    "                                time.sleep(2)\n",
    "                    \n",
    "                            \n",
    "\n",
    "            if row['titleB'] not in pages_titles :\n",
    "                page2 = self.wikipedia.page(row['titleB'])\n",
    "                try:\n",
    "                    page2_content = page2.text\n",
    "                except:\n",
    "                    time.sleep(15)\n",
    "                    print(\"Try again after sleeping for 15 seconds\")\n",
    "                    page2_content = page2.text\n",
    "                if len(page2_content) > 1000: \n",
    "                    pages.append({\"title\": page2.title, \"lang\": \"en\", \"content\": self.clean_text(page2_content)})\n",
    "                    pages_titles.append(row['titleB'])\n",
    "                    soup = BeautifulSoup(urlopen(page2.fullurl))\n",
    "                    links = [(el.get('lang'), el.get('title')) for el in soup.select('li.interlanguage-link > a')]\n",
    "                    for lang, title in links:\n",
    "                        if lang in self.wiki_languages:\n",
    "                            page_title = title.split(u' – ')[0]\n",
    "                            wiki1 = wikipediaapi.Wikipedia(language=lang,extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "                            page = wiki1.page(page_title)\n",
    "                            try:\n",
    "                                p_content = page.text\n",
    "                            except:\n",
    "                                time.sleep(15)\n",
    "                                print(\"Try again after sleeping for 15 seconds\")\n",
    "                                p_content = page.text\n",
    "                            if len(p_content)>1000:\n",
    "                                pages.append({\"title\": page2.title,\"lang\": lang, \"content\": p_content})\n",
    "                                time.sleep(2)\n",
    "        self.save_data_in_file(pages, dataset_name)\n",
    "        return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n"
     ]
    }
   ],
   "source": [
    "# run the code for each dataset\n",
    "wikisrs_sim = LoadingWikiPages(dataset_name=\"WikiSRS_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n",
      "Try again after sleeping for 15 seconds\n"
     ]
    }
   ],
   "source": [
    "wikisrs_rel = LoadingWikiPages(dataset_name=\"WikiSRS_relatedness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_sim353 = LoadingWikiPages(dataset_name=\"wikipediaSimilarity353\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wikipediaSimilarity353.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
