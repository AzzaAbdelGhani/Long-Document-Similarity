{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtzMVyiM1yH2",
    "outputId": "ee13c8d4-738b-4045-8d08-b0e2b2857e56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wikipedia-api in /home/azza/.local/lib/python3.8/site-packages (0.5.4)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from wikipedia-api) (2.22.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install wikipedia-api "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0r818twrgOJ",
    "outputId": "c5643e5b-26f6-4efa-da53-f622198a4d4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/azza/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "import numpy\n",
    "import wikipediaapi\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadingWikiPages() : \n",
    "    def __init__(self, dataset_name = None):\n",
    "        self.headers = requests.utils.default_headers()\n",
    "        self.headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "        self.wikipedia = wikipediaapi.Wikipedia(language='en',extract_format=wikipediaapi.ExtractFormat.WIKI, headers=self.headers)\n",
    "        self.dataset = self.read_data_file(dataset_name)\n",
    "        self.wiki_languages = ['en','ar','es','de','ja','fr','fa','pl']\n",
    "        self.wiki_pages = self.get_wikipedia_pages(dataset_name)\n",
    "\n",
    "    def read_data_file(self, dataset_name):\n",
    "        if dataset_name == \"wikipediaSimilarity353\":\n",
    "            data = pd.read_csv(\"wikipediaSimilarity353.csv\")\n",
    "            data['titleA'] = data['titleA'].replace(['Production, costs, and pricing'],'Production')#no wikipedia page for 'Production, costs, and pricing'\n",
    "        elif dataset_name == \"WikiSRS_relatedness\" or dataset_name == \"WikiSRS_similarity\":\n",
    "            data = pd.read_csv(dataset_name+\".csv\", sep='\\t') \n",
    "            data = data.drop(['RawScores', 'StdDev'], axis = 1)\n",
    "            data.rename(columns = {'Term1':'termA', 'String1':'titleA',\n",
    "                                   'Term2':'termB', 'String2':'titleB',\n",
    "                                   'Mean' :'relatedness'}, inplace = True)\n",
    "        return data\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(\"\\n\", \"\", text)\n",
    "        text = re.sub(\"\\'\", \"\", text)\n",
    "        new_text = re.sub('[!*(-);\\':\"]', \"\", text)\n",
    "        return new_text\n",
    "\n",
    "    def save_data_in_file(self, dataset, dataset_name):\n",
    "        data = pd.DataFrame(dataset)\n",
    "        data.to_csv('multilingual_'+dataset_name+'.csv')\n",
    "        return\n",
    "    \n",
    "    def get_wikipedia_pages(self, dataset_name):\n",
    "        pages = []\n",
    "        pages_titles = []\n",
    "        for index, row in self.dataset.iterrows():\n",
    "            if row['titleA'] not in pages_titles :\n",
    "                page1 = self.wikipedia.page(row['titleA'])\n",
    "                try:\n",
    "                    page1_content = page1.text\n",
    "                except:\n",
    "                    time.sleep(15)\n",
    "                    print(\"Try again after sleeping for 15 seconds\")\n",
    "                    page1_content = page1.text\n",
    "                if len(page1_content) > 1000: \n",
    "                    pages.append({\"title\": page1.title, \"lang\": \"en\", \"content\": self.clean_text(page1_content)})\n",
    "                    pages_titles.append(row['titleA'])\n",
    "                    soup = BeautifulSoup(urlopen(page1.fullurl))\n",
    "                    links = [(el.get('lang'), el.get('title')) for el in soup.select('li.interlanguage-link > a')]\n",
    "                    for lang, title in links:\n",
    "                        if lang in self.wiki_languages:\n",
    "                            page_title = title.split(u' – ')[0]\n",
    "                            wiki1 = wikipediaapi.Wikipedia(language=lang,extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "                            page = wiki1.page(page_title)\n",
    "                            try:\n",
    "                                p_content = page.text\n",
    "                            except:\n",
    "                                time.sleep(15)\n",
    "                                print(\"Try again after sleeping for 15 seconds\")\n",
    "                                p_content = page.text\n",
    "                            if len(p_content)>1000:\n",
    "                                pages.append({\"title\": page1.title,\"lang\": lang, \"content\": p_content})\n",
    "                                time.sleep(2)\n",
    "                    \n",
    "                            \n",
    "\n",
    "            if row['titleB'] not in pages_titles :\n",
    "                page2 = self.wikipedia.page(row['titleB'])\n",
    "                try:\n",
    "                    page2_content = page2.text\n",
    "                except:\n",
    "                    time.sleep(15)\n",
    "                    print(\"Try again after sleeping for 15 seconds\")\n",
    "                    page2_content = page2.text\n",
    "                if len(page2_content) > 1000: \n",
    "                    pages.append({\"title\": page2.title, \"lang\": \"en\", \"content\": self.clean_text(page2_content)})\n",
    "                    pages_titles.append(row['titleB'])\n",
    "                    soup = BeautifulSoup(urlopen(page2.fullurl))\n",
    "                    links = [(el.get('lang'), el.get('title')) for el in soup.select('li.interlanguage-link > a')]\n",
    "                    for lang, title in links:\n",
    "                        if lang in self.wiki_languages:\n",
    "                            page_title = title.split(u' – ')[0]\n",
    "                            wiki1 = wikipediaapi.Wikipedia(language=lang,extract_format=wikipediaapi.ExtractFormat.WIKI)\n",
    "                            page = wiki1.page(page_title)\n",
    "                            try:\n",
    "                                p_content = page.text\n",
    "                            except:\n",
    "                                time.sleep(15)\n",
    "                                print(\"Try again after sleeping for 15 seconds\")\n",
    "                                p_content = page.text\n",
    "                            if len(p_content)>1000:\n",
    "                                pages.append({\"title\": page2.title,\"lang\": lang, \"content\": p_content})\n",
    "                                time.sleep(2)\n",
    "        self.save_data_in_file(pages, dataset_name)\n",
    "        return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = LoadingWikiPages(dataset_name=\"WikiSRS_similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2 = LoadingWikiPages(dataset_name=\"WikiSRS_relatedness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try again after sleeping for 15 seconds\n"
     ]
    }
   ],
   "source": [
    "D3 = LoadingWikiPages(dataset_name=\"wikipediaSimilarity353\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "wikipediaSimilarity353.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
